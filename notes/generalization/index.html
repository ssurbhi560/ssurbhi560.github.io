<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><style>:root{--accent-color:#8ab9f1}</style><title>Generalization in Deep Learning</title><meta name=description content="This is the place where I write occasionally."><meta name=keywords content><meta property="og:url" content="https://ssurbhi560.github.io/notes/generalization/"><meta property="og:type" content="website"><meta property="og:title" content="Generalization in Deep Learning"><meta property="og:description" content="This is the place where I write occasionally."><meta property="og:image" content><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Generalization in Deep Learning"><meta name=twitter:description content="This is the place where I write occasionally."><meta property="twitter:domain" content="https://ssurbhi560.github.io/notes/generalization/"><meta property="twitter:url" content="https://ssurbhi560.github.io/notes/generalization/"><meta name=twitter:image content><link rel=canonical href=https://ssurbhi560.github.io/notes/generalization/><link rel=stylesheet type=text/css href=https://ssurbhi560.github.io//css/normalize.min.css media=print onload='this.media="all"'><link rel=stylesheet type=text/css href=https://ssurbhi560.github.io//css/main.css><link disabled id=dark-theme rel=stylesheet href=https://ssurbhi560.github.io//css/dark.css><script src=https://ssurbhi560.github.io//js/svg-injector.min.js></script><script src=https://ssurbhi560.github.io//js/feather-icons.min.js></script><script src=https://ssurbhi560.github.io//js/main.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.16/dist/katex.min.css integrity=sha384-6LkG2wmY8FK9E0vU9OOr8UvLwsaqUg9SETfpq4uTCN1agNe8HRdE9ABlk+fVx6gZ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.16/dist/katex.min.js integrity=sha384-31El76TwmbHj4rF9DyLsygbq6xoIobG0W+jqXim+a3dU9W53tdH3A/ngRPxOzzaB crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.16/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script></head><body><script type=text/javascript>setThemeByUserPref()</script><header class=header><nav class=header-nav><div class=nav-title><a class=nav-brand href=https://ssurbhi560.github.io/><b>Surbhi Sharma</b></a></div><div class=nav-links><div class=nav-link><a href=https://ssurbhi560.github.io/about>About</a></div><div class=nav-link><a href=https://ssurbhi560.github.io/blogs/>Blogs</a></div><div class=nav-link><a href=https://ssurbhi560.github.io/notes/>Notes</a></div><div class=nav-link><a href=https://ssurbhi560.github.io/index.xml><span data-feather=rss></span></a></div><span class=nav-icons-divider></span><div class="nav-link dark-theme-toggle"><a><span id=theme-toggle-icon data-feather=moon></span></a></div><div class=nav-link id=hamburger-menu-toggle><a><span data-feather=menu></span></a></div><ul class="nav-hamburger-list visibility-hidden"><li class=nav-item><a href=https://ssurbhi560.github.io/about>About</a></li><li class=nav-item><a href=https://ssurbhi560.github.io/blogs/>Blogs</a></li><li class=nav-item><a href=https://ssurbhi560.github.io/notes/>Notes</a></li><li class=nav-item><a href=https://ssurbhi560.github.io/index.xml><span data-feather=rss></span></a></li><li class="nav-item dark-theme-toggle"><a><span id=theme-toggle-icon data-feather=moon></span></a></li></ul></div></nav></header><main id=content><div class="post container"><div class=post-header-section><h1>Generalization in Deep Learning</h1></div><div class=post-content><p><p><em>These are the notes from <strong>Roger Grosse&rsquo;s lecture on generalization in deep learning</strong>.</em></p><ul><li><p>In our ML models we don&rsquo;t want them to learn to model the training data, but rather to <em>generalize</em> to the data it hasn&rsquo;t senn before.</p></li><li><p>The way to measure this <em>Generalization performance</em> thorugh a held-out test set, that has unseen data/examples.</p></li><li><p>Overfitting: If the algorithm works well on the training set but fails to generalize on the test set, we say that it has overfitted to the training data.</p></li><li><p>Some strategies to improve generalization and reducing overfitting :</p><ol><li>Reducing the capacity of the model</li><li>Early Stopping</li><li>Regularization and Weight Decay</li><li>Ensemble Methods</li><li>Input Transformations / Data Augmentation</li><li>Stochastic Regularization</li></ol></li></ul><h3 id=measuring-generalization>Measuring Generalization:</h3><ul><li><p>Cost Function : The average loss over the entire training set
$$
\frac{1}{N} \sum_{i=1}^{N} \mathcal{L}(y(x^{(i)}), t^{(i)})
$$</p></li><li><p>We partition our data into three subsets to measure network&rsquo;s generalization (and there are other reasons as well):</p><ul><li>A <strong>Training Set</strong>, a set of training examples the network is trained on.</li><li>A <strong>Validation Set</strong>, which is used to tune <em>hyperparameters</em> such as number of hidden units, or the learning rate.</li><li>A <strong>Test Set</strong>, which is used to measure the <strong>generalization performance</strong></li></ul></li></ul><p><em>There are various ways to perform this partitioning, one of which is cross-validation. I will write separate notes on that.</em></p><ul><li>We can&rsquo;t tune hyperparameters on the training set, because we want to choose values that will generalize.</li><li>We can&rsquo;t also tune hyperparameters on the test set, because that would be &ldquo;cheating&rdquo;, and we are only allowed to use the test set once, to report the final perfomance.</li></ul><h4 id=the-most-basic-strategy-for-tuning-hyperparameters-is->The most basic strategy for tuning hyperparameters is :</h4><ol><li><p><strong>Grid Search</strong>: In grid search, for each hyperparameter, choose a set of candidate values. Separately train the models using all possible combinations of these values, and choose whiever confifuration gives the best validation erorr (minimum validation error).</p></li><li><p><strong>Random Search</strong>: In random search, we train a bunch of networks uisng random configurations of the hyperparameters, and pick whichever one has the best validation error (minimum)</p></li></ol><p><em>Random search</em> is advantageous over <em>grid search</em> when only a few of many hyperparameters (e.g., 2 out of 10) are important. While <em>grid search</em> becomes infeasible in high dimensions, <em>random search</em> can still effectively explore the space of the important hyperparameters. However, <em>grid search</em> is easier to reproduce, which is beneficial in scientific settings.</p><h3 id=reasoning-about-generalization>Reasoning about generalization</h3><ul><li><p>One reason for overfitting is that the training set contains accidental irregularities. For example, in the case of classifying handwritten digits, all training examples of the digit &lsquo;9&rsquo; might have pixel 122 turned on, while all others have it off. The network may mistakenly learn this as a useful feature.</p></li><li><p><strong>Capacity</strong> of a model is the ability to remember information about its training data. It is roughly the number of parameters (i.e., weights)</p></li></ul><ol><li><p><strong>Effect of Training Set Size:</strong> More training data improves generalization because it&rsquo;s more likely that test examples resemble training examples. It also reduces the impact of accidental regularities, forcing the model to learn true patterns. If test error increases with more data, it likely indicates a bug or model issue. However, training error typically increases with more data, as larger datasets are harder to memorize.</p><ul><li>Training vs. Generalization Error: As training set size increases, training error goes up and generalization error (or test error) goes down, eventually converging.</li></ul></li><li><p><strong>Effect of Model Capacity (Number of Parameters):</strong> Increasing parameters reduces training error by helping the model fit both true and accidental patterns. But generalization error behaves non-monotonicallyâ€”decreasing at first (as the model learns real patterns), then increasing (as the model starts memorizing noise).</p><ul><li>Ideal Model Capacity: The goal is to build models with just enough capacity to learn meaningful regularities in the data without memorizing or overfitting to accidental ones.</li></ul></li></ol><p><img src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fanalyticsindiamag.com%2Fwp-content%2Fuploads%2F2021%2F10%2Fgraph.png&amp;f=1&amp;nofb=1&amp;ipt=15fd8430dcf06a18bbeab59e59c3d0b971880c0ecce6f222a5f139dd01355c48&amp;ipo=images" alt="Qualitative relationship b/w 1. number of training examples and training and test eror. 2. the number of parameters (model capacity) and training and test error"></p><h3 id=bais-and-variance>Bais and Variance</h3><p>&mldr;</p><h3 id=improving-generalization-and-reducing-overfitting>Improving Generalization and Reducing Overfitting</h3><p>&mldr;</p><h4 id=1-reducing-capacity>1. Reducing Capacity</h4><p>&mldr;</p><h4 id=2-early-stopping>2. Early Stopping</h4><p>&mldr;.</p><h4 id=3-regularization-and-weight-decay>3. Regularization and Weight Decay</h4><p>&mldr;</p><h4 id=4-ensemble-methods>4. Ensemble Methods</h4><p>&mldr;.</p><h4 id=5-input-transformations--data-augmentation>5. Input Transformations / Data Augmentation</h4><p>&mldr;.</p><h4 id=6-stochastic-regularization>6. Stochastic Regularization</h4><p>&mldr;.</p></p></div></div></main><footer class=footer><span>&copy; 2025 <a target=_blank href=https://ssurbhi560.github.io>Surbhi Sharma </a></span><span>Made using <a target=_blank href=https://gohugo.io/>Hugo</a> and <a target=_blank href=https://github.com/526avijitgupta/gokarna>Gokarna</a> with some modifications.</span></footer></body></html>