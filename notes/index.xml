<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Surbhi Sharma</title><link>https://ssurbhi560.github.io/notes/</link><description>Recent content on Surbhi Sharma</description><generator>Hugo</generator><language>en</language><lastBuildDate>Tue, 06 Dec 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://ssurbhi560.github.io/notes/index.xml" rel="self" type="application/rss+xml"/><item><title>RNN</title><link>https://ssurbhi560.github.io/notes/rnn/</link><pubDate>Tue, 06 Dec 2022 00:00:00 +0000</pubDate><guid>https://ssurbhi560.github.io/notes/rnn/</guid><description>&lt;p>&lt;em>These are notes from &lt;strong>Roger Grosse&amp;rsquo;s material on Recurrent Neural Networks&lt;/strong>, along with some of my own additions to help me remember the material better.&lt;/em>&lt;/p></description></item><item><title>Generalization in Deep Learning</title><link>https://ssurbhi560.github.io/notes/generalization/</link><pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate><guid>https://ssurbhi560.github.io/notes/generalization/</guid><description>&lt;p>&lt;em>These are the notes from &lt;strong>Roger Grosse&amp;rsquo;s lecture on generalization in deep learning&lt;/strong>.&lt;/em>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>In our ML models we don&amp;rsquo;t want them to learn to model the training data, but rather to &lt;em>generalize&lt;/em> to the data it hasn&amp;rsquo;t senn before.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The way to measure this &lt;em>Generalization performance&lt;/em> thorugh a held-out test set, that has unseen data/examples.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Overfitting: If the algorithm works well on the training set but fails to generalize on the test set, we say that it has overfitted to the training data.&lt;/p></description></item></channel></rss>